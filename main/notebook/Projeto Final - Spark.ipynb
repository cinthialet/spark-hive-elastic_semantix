{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"projeto_final\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando diretório pai HDFS para o projeto \n",
    "!hdfs dfs -mkdir -p /projeto-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/projeto-final/dados': File exists\r\n"
     ]
    }
   ],
   "source": [
    "# criando diretório para receber os arquivos de dados\n",
    "!hdfs dfs -mkdir /projeto-final/dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-31 14:17 /hbase\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-26 13:37 /home\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-19 13:05 /projeto-final\r\n",
      "drwxrwxr-x   - root supergroup          0 2023-07-24 02:44 /tmp\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-16 15:20 /user\r\n"
     ]
    }
   ],
   "source": [
    "# Listando diretórios pai no HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv': File exists\n",
      "put: `/projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv': File exists\n",
      "put: `/projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv': File exists\n",
      "put: `/projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# Enviando arquivos para hdfs na pasta criada\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /projeto-final/dados\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /projeto-final/dados\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /projeto-final/dados\n",
    "!hdfs dfs -put HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /projeto-final/dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   2 root supergroup   62492959 2023-07-15 00:14 /projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   76520681 2023-07-15 00:18 /projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup   91120916 2023-07-15 00:18 /projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   2 root supergroup    3046774 2023-07-15 00:18 /projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "# checando arquivos enviados na pasta alvo\n",
    "!hdfs dfs -ls /projeto-final/dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar cada arquivo CSV em um DataFrame\n",
    "df1 = spark.read.csv(\"/projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\", header=True, sep=';')\n",
    "df2 = spark.read.csv(\"/projeto-final/dados/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\", header=True, sep=';')\n",
    "df3 = spark.read.csv(\"/projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\", header=True, sep=';')\n",
    "df4 = spark.read.csv(\"/projeto-final/dados/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\", header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arquivo 1 :  714481\n",
      "arquivo 2 :  859708\n",
      "arquivo 3 :  1017040\n",
      "arquivo 3 :  33714\n",
      "Total: 2624943\n"
     ]
    }
   ],
   "source": [
    "# checand quantidade de registros a serem carregados\n",
    "print('arquivo 1 : ', df1.count())\n",
    "print('arquivo 2 : ', df2.count())\n",
    "print('arquivo 3 : ', df3.count())\n",
    "print('arquivo 3 : ', df4.count())\n",
    "print('Total:', df1.count() + df2.count() + df3.count() + df4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando os DataFrames em um único DataFrame spark\n",
    "df_total = df1.union(df2).union(df3).union(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: string (nullable = true)\n",
      " |-- codmun: string (nullable = true)\n",
      " |-- codRegiaoSaude: string (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: string (nullable = true)\n",
      " |-- populacaoTCU2019: string (nullable = true)\n",
      " |-- casosAcumulado: string (nullable = true)\n",
      " |-- casosNovos: string (nullable = true)\n",
      " |-- obitosAcumulado: string (nullable = true)\n",
      " |-- obitosNovos: string (nullable = true)\n",
      " |-- Recuperadosnovos: string (nullable = true)\n",
      " |-- emAcompanhamentoNovos: string (nullable = true)\n",
      " |-- interior/metropolitana: string (nullable = true)\n",
      "\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checando schema e primeiros 5 registros\n",
    "df_total.printSchema()\n",
    "df_total.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2624943"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checando se todos os registros foram carregados após union\n",
    "df_total.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popular a tabelsla Hive criada  com os dados do dataframe spark #demorou 25 min p/popular\n",
    "df_total.write.partitionBy(\"municipio\").saveAsTable(\"covid_por_local\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-31 11:36 /user/hive/warehouse/covid19_recuperados_acompanhamento\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-07-26 00:48 /user/hive/warehouse/covid_por_local\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|regiao|estado|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|municipio|\n",
      "+------+------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "|Brasil|  null|   76|  null|          null|           null|2021-01-01|       53|       210147125|       7700578|     24605|         195411|        462|         6756284|               748883|                  null|     null|\n",
      "|Brasil|  null|   76|  null|          null|           null|2021-01-02|       53|       210147125|       7716405|     15827|         195725|        314|         6769420|               751260|                  null|     null|\n",
      "+------+------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"covid_por_local\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualizacao 1 - KPI de \"casos recuperados\" e KPI \"em acompanhamento\"\n",
    "#Calcular os KPIs\n",
    "kpi_recuperados = df_total.agg(sum(\"recuperadosNovos\")).collect()[0][0]\n",
    "kpi_em_acompanhamento = df_total.agg(sum(\"emAcompanhamentoNovos\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualização 1 - KPIs de Casos Recuperados e Em Acompanhamento:\n",
      "Casos Recuperados: 2920055795.0\n",
      "Em Acompanhamento: 320793426.0\n"
     ]
    }
   ],
   "source": [
    "#Exibir os KPIs\n",
    "print(\"Visualização 1 - KPIs de Casos Recuperados e Em Acompanhamento:\")\n",
    "print(\"Casos Recuperados:\", kpi_recuperados)\n",
    "print(\"Em Acompanhamento:\", kpi_em_acompanhamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualização 2 -\"CASOS CONFIRMADOS\" com KPIs \"acumulados\", \"casos novos\" e \"incidência\"\n",
    "#Calcular os KPIs\n",
    "kpi_acumulados = df_total.agg(sum(\"casosAcumulado\")).collect()[0][0]\n",
    "kpi_casos_novos = df_total.agg(avg(\"casosNovos\")).collect()[0][0]\n",
    "kpi_incidencia = kpi_casos_novos / kpi_acumulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualização 2 - CASOS CONFIRMADOS:\n",
      "Acumulados: 9998472092.0\n",
      "Casos Novos: 21.54905649379815\n",
      "Incidência: 2.1552349494519297e-09\n"
     ]
    }
   ],
   "source": [
    "# Exibir os KPIs\n",
    "print(\"Visualização 2 - CASOS CONFIRMADOS:\")\n",
    "print(\"Acumulados:\", kpi_acumulados)\n",
    "print(\"Casos Novos:\", kpi_casos_novos)\n",
    "print(\"Incidência:\", kpi_incidencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualização 3-\"ÓBITOS CONFIRMADOS\" com KPIs \"óbitos acumulados\", \"casos novos\", \"letalidade\" e \"mortalidade\":\n",
    "#Calcular os KPIs\n",
    "kpi_obitos_acumulados = df_total.agg(sum(\"obitosAcumulado\")).collect()[0][0]\n",
    "kpi_obitos_novos = df_total.agg(avg(\"obitosNovos\")).collect()[0][0]\n",
    "kpi_letalidade = (kpi_obitos_acumulados / kpi_acumulados) * 100\n",
    "kpi_mortalidade = kpi_obitos_acumulados / kpi_acumulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualização 3 - ÓBITOS CONFIRMADOS:\n",
      "Óbitos Acumulados: 274784085.0\n",
      "Óbitos Novos: 0.6021753615221359\n",
      "Letalidade (%): 2.74826075895997\n",
      "Mortalidade: 0.0274826075895997\n"
     ]
    }
   ],
   "source": [
    "# Exibir os KPIs\n",
    "print(\"Visualização 3 - ÓBITOS CONFIRMADOS:\")\n",
    "print(\"Óbitos Acumulados:\", kpi_obitos_acumulados)\n",
    "print(\"Óbitos Novos:\", kpi_obitos_novos)\n",
    "print(\"Letalidade (%):\", kpi_letalidade)\n",
    "print(\"Mortalidade:\", kpi_mortalidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar a primeira visualização como tabela Hive\n",
    "df_total.select(sum(\"recuperadosNovos\").alias(\"totalRecuperadosNovos\"), sum(\"emAcompanhamentoNovos\").alias(\"totalEmAcompanhamentoNovos\")).write.saveAsTable(\"covid19_recuperados_acompanhamento\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------------+\n",
      "|totalRecuperadosNovos|totalEmAcompanhamentoNovos|\n",
      "+---------------------+--------------------------+\n",
      "|        2.920055795E9|              3.20793426E8|\n",
      "+---------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checando\n",
    "spark.read.table(\"covid19_recuperados_acompanhamento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar a segunda visualização como formato Parquet com compressão Snappy\n",
    "# Criar DataFrame com os KPIs da segunda visualização\n",
    "kpi_visualizacao2 = spark.createDataFrame([(kpi_acumulados, kpi_casos_novos, kpi_incidencia)], [\"casos_acumulados\", \"casos_novos\", \"incidencia\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Salvar como formato Parquet com compressão Snappy\n",
    "kpi_visualizacao2.write.parquet(\"user/kpi_visualizacao2.parquet\", compression=\"snappy\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+--------------------+\n",
      "|casos_acumulados|      casos_novos|          incidencia|\n",
      "+----------------+-----------------+--------------------+\n",
      "|   9.998472092E9|21.54905649379815|2.155234949451929...|\n",
      "+----------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_viz2 = spark.read.parquet(\"user/kpi_visualizacao2.parquet\")\n",
    "df_viz2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar visualicao 3 no topipo no kafka\n",
    "#Criação do tópico kafka\n",
    "# Feito no terminal : kafka-topics --bootstrap-server localhost:9092 --topic obitoscvd19 --create --partitions 3 --replication-factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Criar a visualização pelo Spark com os dados enviados para o HDFS:\n",
    "#Síntese de casos, óbitos, incidência de mortalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+--------+----------+-----------+-----------+\n",
      "|      regiao|          municipio|      casos|  obitos|incidencia|mortalidade|atualizacao|\n",
      "+------------+-------------------+-----------+--------+----------+-----------+-----------+\n",
      "|      Brasil|               null|1.8855015E7|526892.0|    8972.3|      250.7| 2021-07-06|\n",
      "|Centro-Oeste|               null|  1916619.0| 49207.0|      null|       null| 2021-07-06|\n",
      "|    Nordeste|               null|  4477181.0|108088.0|      null|       null| 2021-07-06|\n",
      "|       Norte|               null|  1735433.0| 43860.0|      null|       null| 2021-07-06|\n",
      "|     Sudeste|               null|  7148156.0|245851.0|      null|       null| 2021-07-06|\n",
      "|         Sul|               null|  3617963.0| 80879.0|      null|       null| 2021-07-06|\n",
      "|Centro-Oeste|    Abadia de Goiás|     1508.0|    34.0|   17189.1|      387.6| 2021-07-06|\n",
      "|     Sudeste|Abadia dos Dourados|      444.0|    14.0|    6352.8|      200.3| 2021-07-06|\n",
      "|Centro-Oeste|          Abadiânia|      443.0|    34.0|    2210.4|      169.6| 2021-07-06|\n",
      "|       Norte|         Abaetetuba|     8894.0|   219.0|    5639.9|      138.9| 2021-07-06|\n",
      "|     Sudeste|             Abaeté|     1313.0|    29.0|    5650.5|      124.8| 2021-07-06|\n",
      "|    Nordeste|            Abaiara|      586.0|    15.0|    4992.8|      127.8| 2021-07-06|\n",
      "|    Nordeste|              Abaré|      971.0|    11.0|    4834.2|       54.8| 2021-07-06|\n",
      "|         Sul|             Abatiá|     1043.0|    18.0|   13986.9|      241.4| 2021-07-06|\n",
      "|    Nordeste|             Abaíra|      288.0|    12.0|    3295.6|      137.3| 2021-07-06|\n",
      "|         Sul|      Abdon Batista|      419.0|     4.0|   16348.0|      156.1| 2021-07-06|\n",
      "|       Norte|    Abel Figueiredo|      970.0|    10.0|   13048.2|      134.5| 2021-07-06|\n",
      "|         Sul|       Abelardo Luz|     1529.0|    55.0|    8540.0|      307.2| 2021-07-06|\n",
      "|     Sudeste|         Abre Campo|     1171.0|    15.0|    8703.7|      111.5| 2021-07-06|\n",
      "|    Nordeste|       Abreu e Lima|     3185.0|   230.0|    3185.3|      230.0| 2021-07-06|\n",
      "+------------+-------------------+-----------+--------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casos | Óbitos | Indicencia 100k/hab | Mortalidade | Atualização\n",
    "# Incidência por 100K habitantes Método de cálculo: (casos confirmados * 100.000) / população\n",
    "# Mortalidade -  Método de cálculo: (óbitos * 100.000) / população  -> por 100K habitantes\n",
    "\n",
    "viz_spark = df_total.groupBy('regiao', 'municipio') \\\n",
    "    .agg(\n",
    "        sum('casosNovos').alias('casos'),\n",
    "        sum('obitosNovos').alias('obitos'),\n",
    "        last('data').alias('atualizacao'),\n",
    "        last('populacaoTCU2019').alias('agg_pop'),\n",
    "    ) \\\n",
    "    .withColumn('incidencia', round((col('casos') * 100_000) / col('agg_pop'), 1)) \\\n",
    "    .withColumn('mortalidade', round((col('obitos') * 100_000) / col('agg_pop'), 1)) \\\n",
    "    .select('regiao', 'municipio', 'casos', 'obitos', 'incidencia', 'mortalidade', 'atualizacao') \\\n",
    "    .orderBy(col('municipio').asc_nulls_first(), 'regiao')\n",
    "\n",
    "viz_spark.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar a visualização que foi salva em tópico kafka em um tópico no Elastic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
